\chapter{Conclusions and Future Work}
\label{ch:concl}

This chapter focuses on concluding remarks and future work.  First in Section
\ref{sec:concl}, the main experimental results from each section are brought
together to discuss the bigger picture of this work.  Additionally, there are
areas of the methodology that are simplified and several avenues this work
could pursue, so this is discussed in Section \ref{sec:future}.

\section{Conclusions}
\label{sec:concl}

The main research question that this work addresses is as follows: \textit{How
does the ability to determine forensic-relevant spent nuclear fuel attributes
using machine-learning techniques degrade as less information is available?}. 
The workflow to examine this question takes place in four steps.  

First, the training data is simulated, which provides an array of nuclide
measurements as the features. The prediction parameters are the simulation
inputs: reactor type, burnup, \gls{U235} enrichment, and time since
irradiation.  The reactor types are one of three common commercial reactors:
\gls{PWR}, \gls{BWR}, or \gls{PHWR}.  The burnup is measured in $MWd/MTU$ and
ranges from 0 to about $68,000$ in 21-28 timesteps depending on the reactor
type.  The enrichment is $\%U235$ and is centered around the following
percentages: 0.5, 1.5, 2.0, 3.0, 4.0, 5.0, and 0.711 (natural \gls{U235}
enrichment for the \gls{PHWR}s).  The time since irradiation is measured in
$days$ and ranges from 0 to 6000, or 16 years, in $100-day$ time steps with 61
total steps. Given these simulation inputs, the makeup of 4.5e5 \gls{SNF}
entries are simulated using \gls{ORIGEN} \cite{scale, origen, origenarp}.
These steps are covered in Sections \ref{sec:training1} and
\ref{sec:training2}.

Second, information reduction on the training set is carried out using randomly
injected uniform error or computationally generated gamma spectra. For the
former, the training set is comprised of nuclide masses and the random error is
used to study the robustness of the methodology to artificial noise in the
feature set.  For the latter, the training set is initially comprised of
nuclide activities, and \gls{GADRAS} is used to compute the gamma spectra via
\gls{DRF}s for six detectors \cite{gadras}. Each detector-based training set
undergoes three different methods of processing the gamma spectra. This is
intended to answer the question of whether field-deployable detectors can give
enough information about radionuclides to be able to successfully attribute
\gls{SNF}.  These steps are covered in Sections \ref{sec:inforeduc1} and
\ref{sec:inforeduc2}.

Third, three algorithms, \textit{k}-nearest neighbors, decision trees, and
\gls{MLL} calculations, are used to train models to predict the four reactor
parameters of interest.  The first two are algorithms implemented using the
scikit-learn python \gls{ML} toolkit, and \gls{MLL} is implemented in python
leveraging SciPy and NumPy for fast likelihood calculations \cite{scikit,
scipy, numpy}.  For the scikit algorithms, the hyperparameters governing model
complexity were optimized to minimize the prediction errors.  The scripts
written to run the scikit-learn predictions and the \gls{MLL} calculations were
deployed using \gls{UW}--Madison's \gls{CHTC} resources, the \gls{UW} campus
grid, and the \gls{OSG} \cite{osg07, osg09}.  These steps are covered in
Sections \ref{sec:statmodel1} and \ref{sec:statmodel2}.

Fourth, the prediction errors are evaluated to draw conclusions about the
capability of statistical methods to inform nuclear material attribution with
increasingly less precise material detection techniques.  These steps are
covered in Sections \ref{sec:eval1} and \ref{sec:eval2}.  The results from each
of the two chapters are summarized as follows.

\noindent \textbf{Reactor Parameter Prediction Using Nuclide Masses}

First, the results from the experimental procedure in Chapter \ref{ch:exp1} are
discussed.  This experiment is a demonstration of the methodology with the
"perfect knowledge" of nuclide masses. It first implements the effects of
information reduction by injecting noise into the training set (randomly
applied uniform error) in Sections \ref{sec:randerrA} and \ref{sec:randerrB}.
Next, the impacts of training set size are evaluated to understand the effects
of model generalization in Section \ref{sec:randerrC}. After this, the impacts
of having prior knowledge of the reactor type on the quality of prediction of
the regression cases are studied in Section \ref{sec:randerrD}. Last, the
external testing set, the \gls{SFCOMPO} database, is used to test this
methodology against sometimes complicated real world data sets in Section
\ref{sec:sfcompo}. 

For Sections \ref{sec:randerrA}--\ref{sec:randerrD}, the prediction performance
is measured by using the training set to provide testing samples. The entire
training set is tested at some point; when a test sample is used, it is first
removed from the training set so that there is no exact replica of it in the
training stage. For the scikit-learn algorithms, this is accomplished via
5-fold \gls{CV}, and the \gls{MLL} calculations test one training set entry at
a time.  Impacts of increasing training set error on the four prediction
parameters are covered in Sections \ref{sec:randerrA} and \ref{sec:randerrB}
The \gls{MLL} calculations method is the most resilient to introduced error,
followed by decision trees then \textit{k}-nearest neighbors. The behaviors of
each algorithm's performance degradation for reactor type, burnup, and
enrichment all behave in a similar fashion. But the \textit{k}-nearest
neighbors prediction of time since irradiation has a drastically fast drop that
starts with even 1\% introduced training set error. The prediction performance
at 20\% training set error is used as a baseline for future work. 

In Section \ref{sec:randerrC}, the impacts of fewer training set entries are
implemented to investigate how the algorithms each generalize. Using the
training set that has 5\% error, it was reduced in four steps from its full
size, the lowest size being 20\% of the full training set. Although the
\gls{MLL} calculations perform above the scikit-learn algorithms at most sizes,
the data points at 20\% training set size show \gls{MLL} below one or both of
the scikit-learn algorithms. The takeaway is that while this training set is
more than large enough to achieve good performance from a high variance
approach, the performance may not hold with a different training set design. 

Section \ref{sec:randerrD} shows whether the burnup, enrichment, and time since
irradiation predictions benefit from having the prior knowledge of reactor
type.  The largest improvement is for the \gls{PHWR} (only 1.5\% of training
set) regression cases for the scikit-learn algorithms, whereas there is no
improvement for \gls{MLL} calculations. Because the \gls{BWR} class dominates
the training set, there is modest improvement in \gls{PWR} regression cases as
well for the scikit-learn algorithms.

Last in this chapter is Section \ref{sec:sfcompo}, where the performance of
this methodology using real world test cases os nuclide concentration
measurements via the \gls{SFCOMPO} database is demonstrated.  While the
training set design spans the label space that exists in \gls{SFCOMPO}, there
are many missing measurements from the features in the training set, which is
comprised of 29 nuclide masses.  Because of an imbalanced training set and the
method used to handle the null values, the reactor type classification results
are poor, which translates to the regression cases as well.  The \gls{MLL}
calculations perform the best, but many of the prediction errors are still
large. The techniques for improving the performance are discussed next in the
future work suggestions, Section \ref{sec:future}.

\noindent \textbf{Reactor Parameter Prediction Using Processed Gamma Spectra}

The second experimental procedure is explored in Chapter \ref{ch:exp2}.  This
experiment's purpose is to probe the usefulness of field-deployable detectors
for giving rapid information about presumed \gls{SNF}. The information
reduction is achieved by using computational gamma spectra of various detectors
with decreasing detector energy resolution.  Additionally, the gamma spectra of
each \gls{SNF} entry are processed in three different manners, resulting in
three energy windows lists, denoted by auto, short, and long.  The performance
of the prediction of reactor parameters is measured by using test cases drawn
from the training set, where there is a training set for each detector in this
study.  Again, the testing of the algorithms is accomplished using 5-fold
\gls{CV} for the scikit-learn algorithms, and by removing one test sample at a
time from the training set for the \gls{MLL} calculations.

The reactor type classifcation results are first covered in Section
\ref{sec:exp2_rxtr}.  The \gls{MLL} calculations consistently perform the best
across the algorithms, and the short energy windows list has the best
performance across the energy windows lists.  Most of the misclassications are
\gls{PWR} and \gls{PHWR} being labeled as \gls{BWR}.  The automatic peak
searching results in erratic behavior for the scikit-learn algorithms because
the high energy resolution detectors perform poorly but one of the lower energy
resolution detectors performed very well. It is therefore a promising approach
for the lower energy resolution detectors, and possibly even still the high
energy resolution detectors since in theory one could filter the noise from the
detector peak searches. 

Next, the regression cases are shown in Section \ref{sec:exp2_reg}.  The burnup
predictions for all the algorithms and energy windows lists outperform the
baseline, except in one case of \textit{k}-nearest neighbors being used with
the auto energy windows list.  The enrichment predictions for the set of six
detectors all fell below the baseline, however, and the time since irradiation
predictions all are very close to the baseline, with some points right above it
and some right below it.  The spread of outliers encompassed $3-17\%$ of the
training set depending on the case, and in most cases the magnitude of the
outlier errors is of significant concern. For all three regression cases, there
was little contrast among the three energy windows lists. The auto list had
more erratic behavior, and the short list had a slight but not significant
average performance over the long list. It is possible that even the short list
with 42 features has too high of a dimension for these methods to perform well. 

\noindent \textbf{Hi hello}

Mll performs well bc of training set design. If there were to be a smaller db
with higher simulation fidelity, other algs may do better because there would
be fewer training set entries. Even a few times throughout this work, knn and
dec trees gets close to MLL performance.

The methods in this work have high variance, and so regularization is necessary,
but hyperparameter optimization left me with high variance models, so this
needs to be discussed somewhere. some discussion of feature number might
also be useful w results

\section{Future Work}
\label{sec:future}

\noindent \textbf{Training Set Features}

Increase simulation fidelity

Feature set study: filtering out problematic nuclides, using iso ratios, filtering out noise for auto peak search approach

\noindent \textbf{Statistical Method Optimization}

High variance algorithms, not much optimization work done

Could implement k-best loglike for MLL!

\noindent \textbf{Serial Prediction}

Rxtr type first, then regression cases. 

Implement ROC studies to vary the decision threshold for PHWR/PWRs to avoid
misclassifications as BWR. Goal here is perfector rxtr type classification if
it comes first. 

\noindent \textbf{\gls{SFCOMPO}}

Serial prediction (rxtr type first, then regression cases)

Nulls imputation can be accomplished via kNN separately, but 0s still would
still likely work best for MLL

