\chapter{Conclusions and Future Work}
\label{ch:concl}

This chapter focuses on concluding remarks and future work.  First in Section
\ref{sec:concl}, the main experimental results from each section are brought
together to discuss the bigger picture of this work.  Additionally, there are
areas of the methodology that are simplified and several avenues this work
could pursue, so this is discussed in Section \ref{sec:future}.

\section{Conclusions}
\label{sec:concl}

The main research question that this work addresses is as follows: \textit{How
does the ability to determine forensic-relevant spent nuclear fuel attributes
using machine-learning techniques degrade as less information is available?}. 
The workflow to examine this question takes place in four steps.  

First, the training data is simulated, which provides an array of nuclide
measurements as the features. The prediction parameters are the simulation
inputs: reactor type, burnup, \gls{U235} enrichment, and time since
irradiation.  The reactor types are one of three common commercial reactors:
\gls{PWR}, \gls{BWR}, or \gls{PHWR}.  The burnup is measured in $MWd/MTU$ , the
enrichment is measured in $\%U235$, and the time since irradiation is measured
in $days$. The size of the training set is 4.5e5 \gls{SNF} entries, which were
simulated using \gls{ORIGEN} \cite{scale, origen, origenarp}.  These steps are
covered in Sections \ref{sec:training1} and \ref{sec:training2}.

Second, information reduction on the training set is carried out using randomly
injected uniform error or computationally generated gamma spectra. For the
former, the training set is comprised of nuclide masses and the random error is
used to study the robustness of the methodology to artificial noise in the
feature set.  For the latter, the training set becomes lists of summed energy
windows of the gamma spectra, which were computed from nuclide activity
measurements using \gls{GADRAS} \gls{DRF}s for six detectors \cite{gadras}.
Each detector-based training set undergoes three different methods of
processing the gamma spectra, denoted as auto, short, and long energy windows
lists.  These steps are covered in Sections \ref{sec:inforeduc1} and
\ref{sec:inforeduc2}.

Third, three algorithms are used to predict the four reactor operation
parameters in this work: \textit{k}-nearest neighbors, decision trees, from the
scikit-learn python \gls{ML} toolkit, and \gls{MLL} calculations, implemented
in python using SciPy and NumPy \cite{scikit, scipy, numpy}.  For the scikit
algorithms, the hyperparameters governing model complexity were optimized to
minimize the prediction errors.  All of the code was run using
\gls{UW}--Madison's \gls{CHTC} resources, the \gls{UW} campus grid, and the
\gls{OSG} \cite{osg07, osg09}.  These steps are covered in Sections
\ref{sec:statmodel1} and \ref{sec:statmodel2}.

Fourth, the prediction errors are evaluated to understand the increase in error
as the information supplied is less precise. This impacts the conclusions that
can be formed about the capability of simple statistical methods to inform
nuclear material attribution.  The testing samples come directly from the
training set, and the prediction errors from  these testing sample are what is
studied for the performance.  When a test sample is used, it is first removed
from the training set so that there is no exact replica of it in the training
stage.  This testing scheme allows the entire training set to be tested
eventually.  It is accomplished via 5-fold \gls{CV} for the scikit-learn
algorithms and by testing one training set entry at a time for the \gls{MLL}
calculations.  These results are presented in Sections \ref{sec:eval1} and
\ref{sec:eval2}, and are summarized as follows.

\noindent \textbf{Reactor Parameter Prediction Using Nuclide Masses}

Here, the results from the experimental procedure in Chapter \ref{ch:exp1} are
discussed.  This experiment is a demonstration of the methodology with the
"perfect knowledge" of nuclide masses. Information reduction is accomplished by
adding noise to the training set in the form of randomly applied uniform error.
The results are in Sections \ref{sec:randerrA} and \ref{sec:randerrB}.  In
order to understand the ability of the models to generalize, the performance
with respect to training set size is studied in Section \ref{sec:randerrC}.
Section \ref{sec:randerrD} introduces the impacts of having prior knowledge of
the reactor type on the quality of prediction of the regression cases.  Last,
the external testing set, the \gls{SFCOMPO} database, is used to test this
methodology against sometimes complicated real world data sets in Section
\ref{sec:sfcompo}. 

Sections \ref{sec:randerrA} and \ref{sec:randerrB} demonstrate the impacts of
injected training set noise on the prediction parameters. Aside from the
\textit{k}-nearest neighbors prediction of time since irradiation, all of the
algorithms follow a similar pattern for each prediction scenario.  The
\gls{MLL} calculations are the most resilient to introduced error, followed by
decision trees then \textit{k}-nearest neighbors.  It is difficult to state
whether or not these results are "good enough" on their own, but they do serve
as a useful baseline for the results in Chapter \ref{ch:exp2}. 

Next, the model generalization is studied in Section \ref{sec:randerrC} by
creating learning curves using $20-100\%$ of the training set.  It shows that
the training set is large enough for the \gls{MLL} calculations to remain the
best performing until the 20\% level, when those data points dip below one or
both of the scikit-learn algorithms depending on the prediction parameter. It
should be noted that this performance is highly dependent on the training set
design because these are qualitatively high variance methods (compared to
common algorithms not shown in this work) that do not generalize well. 

Section \ref{sec:randerrD} investigates the effect of having reactor type prior
knowledge on burnup, enrichment, and time since irradiation prediction.  There
is modest and significant improvement for \gls{PWR} and \gls{PHWR} cases,
respectively, since they are both the minority classes in the training set.
These improvements only exist for the scikit-learn algorithms, since the
\gls{MLL} calculations are not affected by reactor type knowledge.

The last study in this chapter is in Section \ref{sec:sfcompo}, where an
external test set is used that is comprised of measurements from real
commercial power \gls{SNF} from around the world: the \gls{SFCOMPO} database.
The main challenge with this database are that many entries only contain
measurements for uranium and plutonium isotopes, and the missing measurements
are inconsistent. Two methods were implemented: imputing the null values and
replacing the null values with zero.  The reactor type classification results
are very poor except for the \gls{MLL} predictions using zero-valued nulls.
This combination also best predicts the burnup, but oddly, the decision trees
for both methods of null handling best predicts the enrichment.  The
techniques for improving the performance are discussed next in the future work
suggestions, Section \ref{sec:future}.

\noindent \textbf{Reactor Parameter Prediction Using Processed Gamma Spectra}

Next the results of the second experimental procedure from Chapter
\ref{ch:exp2} are discussed.  This experiment's purpose is to probe the
usefulness of field-deployable detectors for giving rapid information about
presumed \gls{SNF}. The information reduction is achieved by using
computational gamma spectra of various detectors with decreasing detector
energy resolution.  The gamma spectra of each \gls{SNF} entry are
processed in three different manners, resulting in three energy windows lists,
denoted by auto, short, and long.  

The reactor type classification results are first covered in Section
\ref{sec:exp2_rxtr}.  Unsurprisingly, the \gls{MLL} calculations consistently
perform the best across the algorithms. There is a slightly better overall
performance by the short energy windows list as compared to the others,
although the large variation by the auto energy windows lists has some data
points performing better.  The confusion matrices show the details of every
data point, but most of the misclassifications are \gls{PWR} and \gls{PHWR} being
labeled as \gls{BWR}.  The automatic peak searching results in erratic behavior
for the scikit-learn algorithms, noting the poor performance of the high energy
resolution detectors, presumably from the added noise of including all peaks
instead of just targeted peaks like the other two lists.  It is still a
promising approach for the lower energy resolution detectors, and possibly even
the high energy resolution detectors since in theory one could filter the noise
from the detector peak searches. 

Next, the regression cases are shown in Section \ref{sec:exp2_reg}.  Regardless
of the energy windows list, the detector-based training sets all do the
following for the \gls{MAPE} and \gls{MAE}: the burnup tends to far outperform
the baseline, the enrichment consistently underperforms with respect to the
baseline, and the time since irradiation has values along the baseline, where
many are below line but close to it.  The spread of absolute errors including
the outliers shows that there is a significant portion of the training set that
is being predicted with very large errors; the spread of outliers encompassed
$3-17\%$ of the training set depending on the case.  Studying these results
with respect to the \gls{MedAE}, the burnup and time since irradiation
predictions do well with respect to their baselines.

\noindent \textbf{Post-Summary Statements}

There is a clear leader throughout all of the results in terms of being a
consistent best performer: the \gls{MLL} calculations. The reason it can
perform well, though, is because of the large number of simulations.  The time
steps in the training set being about $100\:days$ was a limiting factor in its
quality of prediction; many of the \gls{MAE} values were around this level in
the second experiment. The poor enrichment prediction is likely because the
detected radionuclides are not enrichment indicators \todo{check}, and despite
many of the nuclides that are included in the short and long energy windows
lists having known poor simulation quality, the burnup prediction clearly had
enough information to do well.  If there were to be a training set of much
higher fidelity but larger steps between the training set labels, it is
possible \gls{MLL} would not be a top performer. 

The two scikit-learn algorithms still had some interesting variation in their
prediction capabilities despite usually under-performing. Decision trees best
predicted the enrichment for the external testing set scenario with
\gls{SFCOMPO}. For all of the 0\% training set error cases in the first
experiment, the scikit-learn algorithms performed better.  The \gls{MLL}
robustness to error is likely from the different manners in which the error was
applied.  The scikit-learn approach was to change the training set feature
itself to a new value within some range based on the error, but the \gls{MLL}
approach was to include this error instead as uncertainty. In this way, it is
not affecting the features directly, just the resulting uncertainty of the
log-likelihood calculation. If the same approach was used, or some error
existed in the measurements, \gls{MLL} again might not be the best choice. 

Another way in which the implementation differed was the 5-fold \gls{CV} being
used as the testing protocol for the scikit-learn algorithms. For 5 rounds,
20\% of the training set is removed as a testing set, where eventually all 5
folds are tested.  By contrast, the \gls{MLL} test samples get removed a single
entry at a time.  From the learning curves, it appears that reducing the
training set by 20\% would in theory not impact the \gls{MLL} performance, but
it is still a major difference in implementation. 

The methods in this work all have high variance, and so it would have seemed
prudent to apply regularization. However, hyperparameter optimization did not
shift the high variance defaults much towards more bias. \todo{unfinished} 

With respect to the number of features affecting the prediction performance,
the overall similarity of performance among the three energy windows lists
might mean that the feature numbers are not impacting the algorithms much.
Another explanation is that they all include too many features (outside of the
scintillator detectors with the auto energy windows list).  It is possible that
even the short list with 42 features has too high of a dimension for these
methods to perform well. \todo{unfinished}

\section{Future Work}
\label{sec:future}

\noindent \textbf{Training Set Features}

1. Study on badly-simulated nucs, and filtering those out

2. Increase simulation fidelity

3. Feature set study: using iso ratios, filtering out noise for auto peak search approach

\noindent \textbf{Statistical Method Optimization}

High variance algorithms, not much optimization work done

Could implement k-best loglike for MLL!

\noindent \textbf{Serial Prediction}

Rxtr type first, then regression cases. 

Implement ROC studies to vary the decision threshold for PHWR/PWRs to avoid
misclassifications as BWR. Goal here is perfecto rxtr type classification if
it comes first. 

\noindent \textbf{\gls{SFCOMPO}}

Serial prediction (rxtr type first, then regression cases)

Nulls imputation can be accomplished via kNN separately, but 0s still would
still likely work best for MLL

