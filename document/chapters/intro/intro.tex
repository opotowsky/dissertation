
\glsresetall

\chapter{Introduction}
\label{ch:intro}
\todo{edit doc for a/an w/ acronyms}
The realm of nuclear security involves parallel efforts in nonproliferation
(verification of treaty compliance, monitoring for smuggling, proper storage
and transportation of nuclear materials), cyber security, minimizing stocks of
weaponizable materials, disaster response training, and nuclear forensics.
Nuclear forensics is the process by which nuclear material out of regulatory
control is analyzed to provide attribution.  All of these efforts have been
continually improving, but there was an unaddressed gap regarding the ability
of the \gls{US} to coordinate and respond to a nuclear incident, especially
with the technical portion of nuclear forensics: characterization and analysis.
After all, the first textbook on the topic was published in 2005
\cite{nftext_2005}. In 2006, the \gls{US} \gls{DHS} founded the \gls{NTNFC}
within the \gls{DNDO}, with the mission to establish a robust nuclear forensics
capability by attributing radioactive materials with demonstrable proof. This
endeavor was and is highly dependent on inter-agency cooperation. In 2017 \--
2018, the \gls{DNDO} was absorbed into the newer \gls{CWMD} office, and in
2019, much of the nuclear forensics research operation was moved to multiple
offices within the \gls{DOE} \gls{NNSA}. 

There is much overlap between nuclear nonproliferation and safeguards and
nuclear forensics, although they are distinct nuclear security disciplines.
Both fields are concerned with radiological and isotopic measurements of
nuclear materials. So, areas like material collection techniques and detector
technology are broadly beneficial to both fields. However, much of the time,
safeguards must probe a nuclear material or process in a non-destructive manner
or in a way that does not reveal proprietary technology; safeguards has
limitations that nuclear forensics does not.  Therefore, radiochemical
separations (a destructive process for material characterization) and
identifying forensic signatures are areas important to nuclear forensics.
These approaches within nuclear forensics can vary based on whether the
material being collected is pre-detonation (e.g., \gls{SNF}) or post-detonation
(e.g., bomb debris). 

In the pre-detonation realm, this project focuses on statistical methods to to
model the production history of a nuclear material using measurements of
nuclides present in \gls{SNF}.  The \gls{SNF} is identified in this work by
focusing on four characteristics in particular that can potentially provide
material attribution: reactor type, burnup, initial \gls{U235} enrichment, and
time since fuel irradiation. 
\begin{enumerate}
  \item The \textbf{reactor type} is classified as one of the three most common
        types of commercial power reactors: \gls{PWR}, \gls{BWR}, or \gls{PHWR}.
  \item The \textbf{burnup} describes how much energy was produced by the fuel,
        taking on the units megawatt-days (or gigawatt-days) per metric 
        ton of heavy metal (or initial uranium); it is referred to in this work 
        as $MWd/MTU$, and sometimes $GWd/MTU$.  
  \item The \textbf{enrichment} is the percentage of \gls{U235} with respect to
        the entire amount of uranium in the fuel, and is reported as 
        $\%\:{}^{235}\text{U}$. 
  \item Lastly, the \textbf{time since irradiation} is defined as how long the
        fuel has been out of the reactor core, and is measured usually in $days$,
        but sometimes $years$. It may also be referred to as cooling time. 
\end{enumerate}

\section{Motivation}
\label{sec:motivation}

Preventing nuclear terrorism is important work that has a role along the entire
lifetime of nuclear material, from the beginning (e.g., enrichment facility
inspections) to the end (e.g., \gls{SNF} management). The \gls{IAEA} tracks
reports of radioactive material out of regulatory control via the \gls{ITDB}
\cite{itdb}, and the incidents that are confirmed or likely to be related to
trafficking or malicious use are summarized in Figure \ref{fig:incidents}.
These are the circumstances in which a robust nuclear forensics capability is
beneficial.

\begin{figure}[!tbh]
  \makebox[\textwidth][c]{\includegraphics[width=\linewidth]{./chapters/intro/nucleartrafficking.png}}
  \caption[Interdicted nuclear materials for trafficking or malicious use]
          {Incidents reported to the \acrshort{IAEA} \acrshort{ITDB} related to 
          malicious use. Included are highly enriched uranium (12), 
          plutonium (2), plutonium-beryllium neutron sources (5) 
          \cite{itdb}}
  \label{fig:incidents}
\end{figure}

Nuclear forensics is an important aspect of deterring nuclear terrorism even
though it is not, at first glance, obvious preventative nuclear security.  The
most common defense of the field is that nuclear forensics deters state actors,
not terrorist organizations. While it is true that a strong capability
encourages governments to be more active in prevention of nuclear terrorism, it
can also deter the terrorist organizations as well by increasing their chances
of failure. Small destructive successes tend to be more valued than high-risk
mass destruction. Nuclear forensics can also assist in cutting off certain
suppliers of nuclear materials or technologies (e.g., nuclear specialists that
are only involved for financial reasons, access to state suppliers), building a
concrete barrier to nuclear terrorism.  Therefore, nuclear forensics is
considered to impede nuclear terrorism in both tangible and abstract ways
\cite{aps_aaas_forensics}.

Following the prevention value of nuclear forensics, it is important to
understand the process of the technical portion of the investigation and how
that can be improved.  In the event of a nuclear incident, such as the
retrieval of stolen \gls{SNM} or the detonation of a dirty bomb,
it is necessary to learn as much as possible about the source of the materials
in a timely manner. In the case of non-detonated \gls{SNM}, knowing the
processes that produced it is crucial to determine the chain of custody of the
interdicted material.  Section \ref{sec:nfneeds} covers the specific needs of
the nuclear forensics community for \gls{SNF} provenance, and Section
\ref{sec:statscontrib} discusses how computational approaches are useful, with
a focus on why statistical methods in particular are being pursued. 

\subsection{Needs in Nuclear Forensics}
\input{chapters/intro/forensics}
\label{sec:nfneeds}

\subsection{Contribution of Statistical Methods}
\input{chapters/intro/statlearning}
\label{sec:statscontrib}

\section{Methodology}
\label{sec:methodology}

The main goal of the technical portion of a forensics investigation is to
provide a means of nuclear material attribution by taking and analyzing
measurements of the unknown material.  The methodology of this work is based on
this process. The top panel in Figure \ref{fig:nfworkflows} shows an example
technical nuclear forensics workflow as it could occur in the real world for a
pre-detonation scenario.  After a sample is obtained, characterization begins.
With a focus on \gls{SNF}, elemental, chemical, and radiological measurements
are taken.  They are compared to databases filled with previously measured
standard materials with known reactor parameters, and/or the reactor parameters
are calculated from empirical relationships.  These steps might be performed
iteratively in a real investigation, first using non-destructive measurements
(e.g., gamma spectroscopy), and then destructive measurements (e.g., mass
spectrometry).  The reactor parameters could then allow the lookup of reactor
history information, if available, and these results would be provided to
investigators. 

\begin{figure}[!tbh]
  \makebox[\textwidth][c]{\includegraphics[width=\linewidth]{./chapters/intro/ForensicsWorkflows.png}}
  \caption[Comparison of nuclear forensics workflow against research approaches]
          {Schematic comparing nuclear forensics investigative workflow in the 
          top panel with research approaches for physical and computational 
          experiments.}
  \label{fig:nfworkflows}
\end{figure}

Next, the investigation process can be translated to an experimental workflow.
The middle and bottom panels in Figure \ref{fig:nfworkflows} are analogous
physical and computational experiments, respectively.  Both of these
experimental scenarios would have validated measurements of \gls{SNF}; the
middle panel shows this being done in the laboratory and the bottom panel shows
that these are values from a simulation. The goal of an experimental laboratory
study is to test or develop empirical relationships between forensics
signatures and the desired reactor parameters. The goal of computational
studies can be this, finding new empirical relationships, or performing
forensics workflows prior to the implementation of new reactor technologies.
For studying alternative measurement techniques or a slight difference in the
overall approach, a researcher would iterate through multiple studies using
known materials to probe sensitivities or other weaknesses in the procedure.

As mentioned, these processes have informed the experimental design of this
work, which follows the bottom panel in Figure \ref{fig:nfworkflows}. A test
sample of simulated \gls{SNF} will undergo a measurement that is computed using
techniques that mimic destructive or non-destructive measurements of nuclides
present in the sample.  Using a statistical model, the measurements are
compared to a database of \gls{SNF} entries and their reactor parameters,
finding a closest match or prediction for the test sample. Since the test
sample has previously known parameters, the error in the prediction can be
measured and the method can be tuned. 

In addition to the steps of an investigation informing the experimental
protocol, there are other considerations to take into account. In the
simulation and statistical learning paradigm, it is important to determine how
much information to what quality is needed to train an \gls{ML} model. Because
creating databases from real measurements to represent \gls{SNF} from reactor
technologies from around the world is not within the scope of this project, the
database in this study will be created from simulations via the \gls{SCALE}
\cite{scale} system using \gls{ORIGEN} \cite{origen, origenarp}. This is the
first process shown in Figure \ref{fig:intromethod}, referred to as training
data because this is what input information is called in \gls{ML}. It is
covered in Section \ref{sec:training1}.

\begin{figure}[!hbt]
  \makebox[\textwidth][c]{\includegraphics[width=0.3\linewidth]{./chapters/intro/methodology_intro.png}}
  \caption[Experimental methodology flowchart]
          {Flowchart of the steps the experimental methodology for this work.}
  \label{fig:intromethod}
\end{figure}

The second process shown in Figure \ref{fig:intromethod} is information
reduction, and is detailed in Sections \ref{sec:inforeduc1} and
\ref{sec:inforeduc2}.  This refers to measurement error and/or the measurement
type that provides the nuclide information, and allows the extension of this
workflow to better mimic constraints seen in a real-world setting.  For
example, the primary example here is the reduction of information quality via
gamma ray detector, which provide less exact nuclide information than, e.g.,
mass spectrometry.  The radionuclide concentrations from the simulations can be
converted into gamma energies, which then undergo a detector response
calculation to represent real as-measured gamma spectra as closely as possible.
If an algorithm could overcome the limitations of gamma detection and still
provide useful results, this would warrant further studies and perhaps be
field-applicable.

The third step is to choose a method that performs model creation via
statistical learning.  Statistical learners have varied strengths and
weaknesses based on what is being predicted and how they implement
optimization. Introduced in Section \ref{sec:algs} and implemented in Section
\ref{sec:statmodel1} are three methods: \textit{k}-nearest neighbors, decision
trees, and \gls{MLL} calculations. The first two are implemented via a python
\gls{ML} library, scikit-learn \cite{scikit}. The \gls{MLL} method was
originally developed for similar attribution work \cite{mll_method,
mll_sensitivity, mll_validate} and was implemented in python.

After the training is complete, the results of each models' predictions must be
evaluated according to their prediction performance, denoted as the fourth
process in Figure \ref{fig:intromethod}.  The machine-learned model predicts
the parameters of a previously unseen test set.  The difference between the
model predictions and the actual simulated parameters is known as the testing
error or prediction error. This is outlined in Section \ref{sec:testerr}, and
the results are in Sections \ref{sec:eval1} and \ref{sec:eval2}.

Thus, ultimately, this research protocol is designed to answer the question
\textit{How does the ability to determine forensic-relevant spent nuclear fuel
attributes using machine-learning techniques degrade as less information is
available?}. 

\section{Goals}

The main purpose of this work is to evaluate the utility of statistical methods
as an approach to determine nuclear forensics-relevant quantities as less
information is available. \Gls{ML} algorithms (\textit{k}-nearest neighbors,
decision trees, and \gls{MLL} calculations) are used to train models to provide
these categories (reactor type) and values (burnup, enrichment, and time since
irradiation) from the available information.  The training data is simulated,
which provides an array of nuclide measurements as the features ($\vec{X}$).
The prediction parameters of interest ($\vec{y}$) are provided from the
simulation inputs. Information reduction on $\vec{X}$ is carried out using
artificially injected random error or computationally generated gamma spectra.
The prediction errors ($\vec{y}_{true}$ versus $\vec{y}_{pred}$) will be
studied to draw conclusions about the capability of statistical methods to
inform nuclear material attribution with less precise detection techniques.

The necessary background is covered in Chapter \ref{ch:litrev}.  First, an
introduction to the broader field of nuclear forensics is in Section
\ref{sec:nfoverview} to place this work in the context of the technical mission
areas. After that, a short discussion of the field of \gls{ML}, the algorithms
used, and model performance considerations are in Section \ref{sec:mlback}.
Lastly, a review of statistical methods being used in studies of forensics
analysis is covered in Section \ref{sec:stats4nf}. 

After the existing work is discussed, the first experimental procedure is
outlined in Chapter \ref{ch:exp1}: Reactor Parameter Prediction Using Nuclide
Masses.  This experiment is done as a demonstration of the methodology with the
"perfect knowledge" of nuclide masses. These measurements also undergo
information reduction by way of randomly applied uniform error. The performance
of test cases drawn from the training data is presented and discussed, as well
as the performance of real external test cases of nuclide concentration
measurements.  The methodology and implementation of the experimental
components are introduced in four subsections: the simulated training data is
in Section \ref{sec:training1}, the information reduction is in Section
\ref{sec:inforeduc1}, the details for training models are in Section
\ref{sec:statmodel1}, and the performance evaluation is in Section
\ref{sec:eval1}. Within the performance evaluation, the random error injection
results are in Section \ref{sec:randerr} and the performance using a real world
set of test cases in Section \ref{sec:sfcompo}.

Next, the second experimental procedure is explored in Chapter \ref{ch:exp2}:
Reactor Parameter Prediction Using Processed Gamma Spectra.  This experiment's
purpose is to probe the usefulness of field-deployable detectors for giving
rapid information about presumed \gls{SNF}. The information reduction is
achieved by using computational gamma spectra of various detectors with
decreasing detector energy resolution.  The performance of the prediction of
reactor parameters is measured by using test cases drawn from the training set,
where there is a training set for each detector in this study.  The methodology
follows the same workflow as the first experiment, but updates to the
components will be covered: the new training set features are in Section
\ref{sec:training2}, the information reduction from full nuclide knowledge to
processed gamma spectra is in Section \ref{sec:inforeduc2}, the changes to the
training models are in Section \ref{sec:statmodel2}, and the performance
evaluation is in Section \ref{sec:eval2}.

Lastly, the concluding remarks are discussed in Chapter \ref{ch:concl}. After a
summary in Section \ref{sec:summary}, the main conclusions drawn from the results
in Sections \ref{sec:eval1} and \ref{sec:eval2} are covered in Section
\ref{sec:concl}. There are also many avenues that this work does not explore
and several ways to extend this work; these are outlined in Section
\ref{sec:future}.
