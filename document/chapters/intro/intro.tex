\chapter{Introduction}
\label{ch:intro}

The realm of nuclear security involves parallel efforts in nonproliferation
(verification of treaty compliance, monitoring for smuggling, proper storage and
transportation of nuclear materials), cyber security, minimizing stocks of
weaponizable materials, disaster response training, and nuclear forensics. All
of these efforts have been continually improving, but there was a gap regarding
the ability of the \gls{US} to coordinate and respond to a nuclear incident,
especially with the technical portion of nuclear forensics: characterization and
analysis. After all, the first textbook on the topic was published in 2005
\cite{nftext_2005}. In 2006, the \gls{US} \gls{DHS} founded the \gls{NTNFC}
within the \gls{DNDO}, with the mission to establish a robust nuclear forensics
capability by attributing radioactive materials with demonstrable proof. This
endeavor was and is highly dependent on inter-agency cooperation. In 2017 \--
2018, the \gls{DNDO} was absorbed into the newer \gls{CWMD} office, and in 2019,
much of the nuclear forensics research operation was moved to multiple offices
within the \gls{DOE} \gls{NNSA}. 

Multiple fields contribute to a nuclear forensics capability, such as
radiochemical separations, material collection techniques, detector technology,
material library development, and identifying forensic signatures. These needs
vary based on whether the material being collected is post-detonation (e.g.,
bomb debris) or pre-detonation (e.g., \glsreset{SNF}\gls{SNF}).  In the
pre-detonation realm, this project focuses on statistical methods to to model
the production history of a nuclear material using measurements of nuclides
present in the \gls{SNF}.

\gls{SNF} is identified in this work by focusing on four characteristics in
particular that can potentially provide material attribution: reactor type,
burnup, initial \gls{U235} enrichment, and time since fuel irradiation. 
\todo[inline]{need to introduce this early but not sure I like it here}
\begin{enumerate}
  \item The reactor type is classified as one of the three most common types of
  commercial power reactors: \gls{PWR}, \gls{BWR}, or \gls{PHWR}. 
  \item The burnup describes how much energy was produced by the fuel, taking
  on the units megawatt-days (or gigawatt-days) per metric ton of heavy metal
  (or initial uranium); it is referred to in this work as $MWd/MTU$, and
  sometimes $GWd/MTU$.  
  \item The enrichment is the percentage of \gls{U235} with respect to the
  entire amount of uranium in the fuel, and is reported as $\%U235$. 
  \item Lastly, the time since irradiation is defined as how long the fuel has
  been out of the reactor core, and is discussed usually in $days$, but
  sometimes $years$. 
\end{enumerate}

\section{Motivation}
\label{sec:motivation}

Preventing nuclear terrorism is important work that has a role along the entire
lifetime of nuclear material, from the beginning (e.g., enrichment facility
inspections) to the end (e.g., \gls{SNF} management). The \gls{IAEA} tracks
reports of radioactive material out of regulatory control via the \gls{ITDB}
\cite{itdb}, and the incidents that are confirmed or likely to be related to
trafficking or malicious use are summarized in Figure \ref{fig:incidents}.
These are the circumstances in which a robust nuclear forensics capability is
beneficial.

\begin{figure}[!tbh]
  \makebox[\textwidth][c]{\includegraphics[width=\linewidth]{./chapters/intro/nucleartrafficking.png}}
  \caption{Incidents reported to the \acrshort{IAEA} \acrshort{ITDB} related to 
           malicious use. Included are highly enriched uranium (12), 
           plutonium (2), plutonium-beryllium neutron sources (5) 
           \cite{itdb}}
  \label{fig:incidents}
\end{figure}

Nuclear forensics is an important aspect of deterring nuclear terrorism even
though it is not, at first glance, obvious preventative nuclear security.  The
most common defense of the field is that nuclear forensics deters state actors,
not terrorist organizations. While it is true that a strong capability
encourages governments to be more active in prevention of nuclear terrorism, it
can also deter the terrorist organizations as well by increasing their chances
of failure. Small destructive successes tend to be more valued than high-risk
mass destruction. Nuclear forensics can also assist in cutting off certain
suppliers of nuclear materials or technologies (e.g., nuclear specialists that
are only involved for financial reasons, access to state suppliers), building a
concrete barrier to nuclear terrorism.  Therefore, nuclear forensics is
considered to impede nuclear terrorism in both tangible and abstract ways
\cite{aps_aaas_forensics}.

Following the prevention value of nuclear forensics, it is important to
understand the process of the technical portion of the investigation and how
that can be improved.  In the event of a nuclear incident, such as the
retrieval of stolen \glsreset{SNM}\gls{SNM} or the detonation of a dirty bomb,
it is necessary to learn as much as possible about the source of the materials
in a timely manner. In the case of non-detonated \gls{SNM}, knowing the
processes that produced it is crucial to determine the chain of custody of the
interdicted material.  Section \ref{sec:nfneeds} covers the specific needs of
the nuclear forensics community for \gls{SNF} provenance, and Section
\ref{sec:statscontrib} discusses how computational approaches are useful, with
a focus on why statistical methods in particular are being pursued. 

\subsection{Needs in Nuclear Forensics}
\input{chapters/intro/forensics}
\label{sec:nfneeds}

\subsection{Contribution of Statistical Methods}
\input{chapters/intro/statlearning}
\label{sec:statscontrib}

\section{Methodology}
\label{sec:methodology}

The main goal of the technical portion of a forensics investigation is to
provide a means of nuclear material attribution by taking and analyzing
measurements of the unknown material.  The methodology of this work is based on
this process. The top panel in Figure \ref{fig:nfworkflows} shows an example
technical nuclear forensics workflow as it could occur in the real world for a
pre-detonation scenario.  After a sample is obtained, characterization begins.
With a focus on \gls{SNF}, elemental, chemical, and radiological measurements
are taken.  They are compared to databases filled with previously measured
standard materials with known reactor parameters, and/or the reactor parameters
are calculated from empirical relationships.  These steps might be performed
iteratively in a real investigation, first using non-destructive measurements
(e.g., gamma spectroscopy), and then destructive measurements (e.g., mass
spectrometry).  The reactor parameters could then allow the lookup of reactor
history information, if available, and these results would be provided to
investigators. 

\begin{figure}[!tbh]
  \makebox[\textwidth][c]{\includegraphics[width=\linewidth]{./chapters/intro/ForensicsWorkflows.png}}
  \caption{Schematic comparing nuclear forensics investigative workflow in the 
           top panel with research approaches for physical and computational 
           experiments.}
  \label{fig:nfworkflows}
\end{figure}

Next, the investigation process can be translated to an experimental workflow.
The middle and bottom panels in Figure \ref{fig:nfworkflows} are analagous
physical and computational experiments, respectively.  Both of these
experimental scenarios would have validated measurements of \gls{SNF}; the
middle panel shows this being done in the laboratory and the bottom panel shows
that these are values from a simulation. The goal of an experimental laboratory
study is to test or develop empirical relationships between forensics
signatures and the desired reactor parameters. The goal of computational
studies can be this, finding new empirical relationships, or performing
forensics workflows prior to the implementation of new reactor technologies.
For studying alternative measurement techniques or a slight difference in the
overall approach, a researcher would iterate through multiple studies using
known materials to probe sensitivities or other weaknesses in the procedure.

As mentioned, these processes have informed the experimental design of this
work, which follows the bottom panel in Figure \ref{fig:nfworkflows}. A test
sample of simulated \gls{SNF} will undergo a measurement that is computed using
techniques that mimic destructive or non-destructive measurements of nuclides
present in the sample.  Using a statistical model, the measurements are
compared to a database of \gls{SNF} entries and their reactor parameters,
finding a closest match or prediction for the test sample. Since the test
sample has previously known parameters, the error in the prediction can be
measured and the method can be tuned. 

In addition to the steps of an investigation informing the experimental
protocol, there are other considerations to take into account. In the
simulation and statistical learning paradigm, it is important to determine how
much information to what quality is needed to train an \gls{ML} model. Because
creating databases from real measurements to represent \gls{SNF} from reactor
technologies from around the world is not within the scope of this project, the
database in this study will be created from simulations via the \gls{SCALE}
\cite{scale} system using \gls{ORIGEN} \cite{origen}. This is the first process
shown in Figure \ref{fig:intromethod}, referred to as training data because
this is what input information is called in \gls{ML}. It is covered in Section
\ref{sec:snfsim}.

\begin{figure}[!hbt]
  \makebox[\textwidth][c]{\includegraphics[width=0.3\linewidth]{./chapters/intro/methodology_intro.png}}
  \caption{Flowchart of the steps the experimental methodology for this work.}
  \label{fig:intromethod}
\end{figure}

The second process shown in Figure \ref{fig:intromethod} is information
reduction, and is detailed in Section \ref{sec:inforeduc}.  This refers to
measurement error and/or the measurement type that provides the nuclide
information, and allows the extension of this workflow to better mimic
constraints seen in a real-world setting.  For example, the primary example
here is the reduction of information quality via gamma ray detector, which
provide less exact nuclide information than, e.g., mass spectrometry.  The
radionuclide concentrations from the simulations can be converted into gamma
energies, which then undergo a detector response calculation to represent real
as-measured gamma spectra as closely as possible.  If an algorithm could
overcome the limitations of gamma detection and still provide useful results,
this would warrant further studies and perhaps be field-applicable.

The third step is to choose a method that performs model creation via
statistical learning.  Statistical learners have varied strengths and
weaknesses based on what is being predicted and how they implement
optimization. Outlined in Section \ref{sec:algs} are three methods:
\textit{k}-Nearest Neighbors, Decision Trees, and \gls{MLL} calculations. The
first two are implemented via a python \gls{ML} library, scikit-learn
\cite{scikit}. The \gls{MLL} method was originally developed for similar
attribution work in the following studies: \cite{mll_method, mll_sensitivity,
mll_validate}.

After the training is complete, the results of each models' predictions must be
evaluated according to their prediction performance, denoted as the fourth
process in Figure \ref{fig:intromethod}.  The machine-learned model predicts
the parameters of a previously unseen test set.  The difference between the
model predictions and the actual simulated parameters is known as the testing
error or prediction error. \todo[inline]{link section} 

Thus, ultimately, this research protocol is designed to answer the question
\textit{How does the ability to determine forensic-relevant spent nuclear fuel
attributes using machine-learning techniques degrade as less information is
available?}. 

\section{Goals}

The main purpose of this work is to evaluate the utility of statistical methods
as an approach to determine nuclear forensics-relevant quantities as less
information is available. \Gls{ML} algorithms are used to train models to
provide these categories (reactor type) and values (burnup, enrichment, and
time since irradiation) from the available information.  The training data is
simulated using \gls{ORIGEN}, which provides an array of nuclide measurements
as the features ($X$) and the parameters of interest ($y$) are provided from
the simulation inputs. Information reduction is carried out using artificially
injected random error or computationally generated gamma spectra. The
prediction errors ($y_{true}$ versus $y_{pred}$) will be studied to draw
conclusions about the capability of statistical methods to inform nuclear
material attribution with less precise detection techniques.

\todo[inline]{update everything below} 

The necessary background is covered in Chapter \ref{ch:litrev}.  First, an
introduction to the broader field of nuclear forensics is in Section
\ref{sec:nfoverview} to place this work in the context of the technical mission
areas. After that, a short discussion of the field of \gls{ML}, the algorithms
used, and validation methods are in Section \ref{sec:mlback}.  Section
\ref{sec:fcsim} includes information about the software used to generate the
training data and perform the predictions.  Lastly, a review of statistical
methods being used in studies of forensics analysis is covered in Section
\ref{sec:stats4nf}. 

After the existing work is discussed, the methodology and a demonstration of
the experimental components is introduced next in Chapter \ref{ch:method}.
This will cover the simulated training data in Section \ref{sec:training}, the
the details for training models in Section \ref{sec:statmodel}, and the process
of model evaluation in Section \ref{sec:valid}.  

Finally, 
