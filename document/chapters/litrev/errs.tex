After a model is trained, its performance must be evaluated. The following
discusses the considerations taken for the evaluation of the prediction
performance for the algorithms used in this work. 

\gls{ML} algorithms are heavily dependent on the training inputs and algorithm
parameters given to them, such as training set sizes, regularization (defined
below in Section \ref{sec:complexity}), number of features in the training set,
algorithm hyperparameters, etc.  To obtain reliable models, one must both
choose or create a training set carefully and study the impact of various
algorithm parameters on the error. Various error metrics are first covered in
Section \ref{sec:testerr} before the causes of error are discussed in Section
\ref{sec:complexity}.

\subsubsection{Testing Error}
\label{sec:testerr}

The creation of an \gls{ML} model is (usually) a hidden process. Although the
model emerges from a black box, there are ways to evaluate its generalization
(i.e., prediction) capability.  This is done by removing a small portion of the
database for use as a testing set.  The rest of the data set is known as the
training set and is used to train a model. After training, the test set is used
to calculate the model's error to unseen test samples.  This error is typically
referred to as the \textit{testing error}, as it is measuring the ability of
the model to predict future cases that were not introduced in the training
phase. Next, the various metrics used to evaluate classification and regression
are covered.

%In addition to evaluating a single learned model, it is beneficial to compare
%models. Moreover, there are potential degeneracies in the solution space. This
%is because most inverse problems are \textit{ill-posed}, because the solution
%is not guaranteed to be unique \cite{skutnik_2016}.  Evaluating not only the
%solution, but the confidence in the solution, is therefore prudent. While the
%two scikit-implemented algorithms do not provide this information, the
%\gls{MLL} calculation method provides a likelihood with an uncertainty. This
%provides a measure of distinguishability that many machine learning approaches
%do not provide. 

\noindent \textbf{Reactor Type Classification}

For the classification of reactor type, it is typical to use an accuracy score
for classification, where the total number of correct predictions is taken as a
fraction of the entire sample set.
\begin{equation}
  \textit{accuracy} = \frac{1}{N_\text{samples}} \sum_{i=1}^{N_\text{samples}} 
                      \mathbb{I}(y_{pred,i} = y_{true,i})
\end{equation}
Note that $\mathbb{I}(x)$ here is being used to refer to the indicator
function, where when $x$ is true it takes a value of 1 and when $x$ is false it
takes a value of 0.

But training sets can have an uneven number of classes represented.  A more
fair scoring system for imbalanced data sets is instead balanced accuracy,
which averages the accuracy of each class. This would provide a range from 0 to
1. In this work, however, the balanced accuracy is used with
\texttt{adjusted=True} in scikit-learn. This rescales the range to
$\frac{1}{1-N_\text{classes}}$ to 1, where 0 is considered random scoring.
This is done by defining a sample weight as the following, based on its class
frequency $w_j$ \cite{scikit}. This differs from the definition in the
scikit-learn documentation because it presumes a previous sample weight,
whereas this work does not pre-define sample weights.
\begin{equation}
  w_i = \frac{1}{\sum_j{\mathbb{I}(y_j = y_i) w_j}}
\end{equation}
for \textit{j} classes and \textit{i} samples. Thus, the balanced accuracy is
the following with the adjusted weights.
\begin{equation}
  \textit{balanced-accuracy} = \frac{1}{\sum_{i}{w_i}} \sum_{i=1}^{N_\text{samples}}
                               w_i \cdot \mathbb{I}(y_{pred, i} = y_{true_i})
\end{equation}

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.4\linewidth]{./chapters/litrev/cm_example.png}
  \caption[Example of a confusion matrix]
          {Example of a confusion matrix for the three reactor types}
  \label{fig:cm_ex}
\end{figure}

In addition to the metrics which combine the knowledge of true positive and
true negative predictions, it is also important to study the
misclassifications. Confusion matrices show both the true positive predictions
and the false positive predictions, which can be useful information in
understanding an imperfect accuracy or balanced accuracy score. For example,
Figure \ref{fig:cm_ex} pictures an example confusion matrix that could result
from this data set, where a fraction of \glspl{PHWR} are correctly predicted
(0.9), but there are some misclassified as \gls{BWR} (0.1).  While there are no
\glspl{PWR} or \glspl{BWR} misclassified as \gls{PHWR}, they do get
misclassified as each other: 0.3 \glspl{PWR} and 0.1 \glspl{BWR} get
misclassified as the other.  Note that each row must add up to 1, since those
are the true class labels. The columns do not add up to 1 (unless there are no
misclassifications).  

\noindent \textbf{Regression Mean Error Calculations}

For the three regression cases, there are a few metrics that are being
calculated to measure prediction performance: the \gls{MAE}, \gls{MedAE}, and
\gls{MAPE}. The most common metrics to use for comparing regression errors are
\gls{RMSE} or \gls{MAE}, but these mean errors do not provide the full picture.
The \gls{MedAE} can provide interesting insight into a middle-ground or
majority behavior. Also, the relative error via \gls{MAPE} can provide insight
especially when there is a large span of values for a regression case; a large
absolute error could be a small relative error. Therefore, they are all
tracked.  The \gls{MAE}, \gls{MedAE}, and \gls{MAPE} are calculated as follows,
respectively, for each label $j$ (which is suppressed in the math below for
clarity).
\begin{equation}
  \textit{MAE} = \frac{1}{N_{\text{samples}}} \sum_{i=1}^{N_{\text{samples}}} 
                 \left| y_{true, i} - y_{pred, i} \right|
\end{equation}
\begin{equation}
  \textit{MedAE} = \text{median}(\mid y_{true, 1} - y_{pred, 1} \mid, \ldots, 
                                 \mid y_{true, n} - y_{pred, n} \mid)
\end{equation}
\begin{equation}
  \textit{MAPE} =  \frac{100}{N_{\text{samples}}} \cdot 
                   \sum_{i=1}^{N_{\text{samples}}}
                   \frac{\left| y_{true, i} - y_{pred, i} \right|}{y_{true, i}}
\end{equation}

\noindent \textbf{Cross Validation}

A testing set that would be used during training to give feedback, a
\textit{\gls{CV} set}, can provide a faster convergence to a satisfactory
model. As shown in Figure \ref{fig:cverror}, this can be done by splitting the
data set into three groups: a large training set, a small \gls{CV} set, and a
small testing set.  

\begin{figure}[!htb]
  \centering
  \includegraphics[width=0.85\linewidth]{./chapters/litrev/cverror.png}
  \caption[Illustration of cross validation]
          {Illustration of two ways of performing \gls{CV}: a one-time split, 
           or \textit{k}-fold \gls{CV}.}
  \label{fig:cverror}
\end{figure}

However, in practice, multiple rounds of \gls{CV} steps are used provide a
better estimate of model performance by increasing the number of testing sets.
This is referred to as \textit{k-fold cross-validation}.  An example where
$k=3$ is illustrated in Figure \ref{fig:cverror}.  One partition of the
training set is designated as the testing set, and a model is trained with the
rest. This returns the testing error for that first testing partition.
Following the first training phase, another begins, this time with a different
subset as the testing set.  In total, this process is performed three times,
giving three models. Since each partition becomes a testing set at one point,
all entries in the training set are tested, which reduces the chance that the
model is being tested with a misrepresentative testing set. In most
applications, the testing error results from each partition (which are an
average of all test cases in that partition) are averaged to provide a picture
of the model performance. However, this work instead focuses on the aggregate
statistics of all the \textit{individual} test case errors taken together,
regardless of which partition they were in.

\subsubsection{Model Complexity}
\label{sec:complexity}

In statistical learning, there are two sources of error that need to be
simultaneously minimized: bias and variance. Bias is caused by simplifications
in the model, so the error is caused by missed relationships in the data; high
bias is an indication of an underfit model.  Variance is caused by including
random noise in the model, so the error is caused by oversensitivity to that
noise; high variance is an indication of an overfit model. What follows is a
discussion on error considerations that all reduce to one concept: how
\textit{complex should a model be} to best predict a previously unseen test
sample?

\begin{figure}[!htb]
  \makebox[\textwidth][c]{\includegraphics[width=0.7\textwidth]{./chapters/litrev/BVtradeoff.png}}
  \caption[Schematic of the bias-variance trade-off]
          {Schematic showing the sources of error, bias and variance, and how 
           they behave with respect to model complexity in the bias-variance 
           trade-off.}
  \label{fig:bvtradeoff}
\end{figure}

Figure \ref{fig:bvtradeoff} shows the tradeoff between the bias and variance.
The shape of the total error curve has a minimum that we seek to achieve with
our model. Some bias is desired in order to generalize to future unknown data.
But, some variance is positive for the model because it captures the
relationships in the data that the bias counteracts. 

\textit{Regularization} refers to introducing a term into the \gls{ML} model to
prevent overfitting; it is used in many \gls{ML} algorithms to reduce the model
complexity and therefore the resulting variance.  This would be represented by
increasing \textit{k} in \textit{k}-nearest neighbors, or reducing the maximum
features a decision trees implementation could consider for splitting.  The top
three windows in Figure \ref{fig:complex} show the effects of regularization on
a simple linear regression model. With heavy regularization comes high bias,
represented by the left-most window.  The right-most window shows a low
regularization scenario, where most individual points are tracked by the model,
but generalizing beyond that might be problematic. The middle plot represents an 
approximately well-fit model.  

\begin{figure}[!htb]
  \centering
  \makebox[\textwidth][c]{\includegraphics[width=0.9\textwidth]{./chapters/litrev/ValidationCurve.png}}
  \caption[Diagram of model performance with respect to model complexity]
          {Diagram showing effect of model complexity/regularization at three 
           different levels on model performance.}
  \label{fig:complex}
\end{figure}

Diagnostic plots show the testing errors with respect to some variable on the
horizontal axis.  Typically this variable is related in some way to the model
complexity. This provides insight into the model's fitness, and whether small
tweaks can be made to increase bias or variance to improve testing performance.
Put another way, these approaches can evaluate under- or over-fitting.  When
the horizontal axis is related to regularization parameters, often referred to
as algorithm hyperparameters in this work, it is known as a \textit{validation
curve}. A full treatment of validation curves is not considered here, but
Figure \ref{fig:complex} shows the portion relevant to the discussion in the
bottom plot.  The negative prediction error is plotted on the vertical axis
so that the orientation of higher is better is maintained.  A parameter
influencing model complexity is on the horizontal axis.  The testing error is
typically low for the high bias and high variance models, but there usually
exists an optimum parameter for model complexity for most training data sets. 

Another parameter indirectly influencing model complexity is the training set
size.  When the training set size is plotted on the horizontal axis as either
the percentage of the training set used or the absolute number of observations,
it is called a \textit{learning curve}.  These allow for the user to evaluate
the optimum number of training set observations to include in the training
phase.  This is relevant in a scenario like this work where the training set is
large (large being a relative term).  

The training set size must be large and diverse enough to be considered
\gls{i.i.d.} because most \gls{ML} algorithms are developed upon this
assumption. Sometimes this is not possible, and the training data are skewed,
i.e., a portion of the data is over-represented. This must be handled
explicitly, but since each algorithm handles skewed data differently, it is
currently beyond the scope of this work. Instead, attempts were made to best
create an \gls{i.i.d.} training set, which is covered in Section
\ref{sec:snflbls}.

Another area worthy of investigation is the other dimension of the training
set: the number of features included for model training. This is not usually a
value that would be plotted on the horizontal axis of a diagnostic plot (unless
one's data is predisposed to this kind of study), but is considered in this
work. The feature set selection is discussed in Sections \ref{sec:snffeats},
\ref{sec:training2}, and \ref{sec:inforeduc2}.

In practice, plotting learning and validation curves can be iterative. But too
many optimizations will result in a poorly performing model when exposed to
data outside of the training set, so there is a risk associated with better
prediction after using optimization tools.  This increase in performance from
over-optimization could be linked to the training set performance and might not
generalize outside of the specific type of input data used.  A workaround for
this scenario is to obtain more data for the set or to obtain a completely
different data set altogether. 

