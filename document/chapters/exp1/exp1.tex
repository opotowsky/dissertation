
\glsresetall

\chapter{Reactor Parameter Prediction Using Nuclide Masses}
\label{ch:exp1}

This chapter covers the parameter prediction workflow using nuclide masses as
the input features. The methodology is introduced by detailing each
experimental component and its implementation. This is split into four
sections, which correspond to the four steps summarized in Figure
\ref{fig:method1}.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.7\linewidth]{./chapters/exp1/methodology1.png}
  \caption[Experimental methodology in Chapter \ref{ch:exp1}]
          {Flowchart of the experimental methodology and the way each step is 
           being implemented.}
  \label{fig:method1}
\end{figure}

Section \ref{sec:training1} discusses how the training data set is obtained
through simulations and computational means. The initial training data
simulated via \gls{ORIGEN} is detailed in Section \ref{sec:training1} This
provides a set of \gls{SNF} observations with known reactor operation
parameters, i.e., labels that are to be predicted. The four labels being
predicted are as follows:
\begin{enumerate}
  \item The classification of the \textbf{reactor type} is one of the three
        most common commercial power reactors: \gls{PWR}, \gls{BWR}, or 
        \gls{PHWR}.
  \item The \textbf{burnup} describes how much energy was produced by the fuel
        and has the units: $MWd/MTU$ (or $GWd/MTU$), mega (or giga) 
        watt-days per metric ton of initial uranium.
  \item The \textbf{enrichment} is the percentage of \gls{U235} with respect to
        the entire amount of uranium in the fuel: $\%\:{}^{235}\text{U}$. 
  \item Lastly, the \textbf{time since irradiation}, or cooling time, is
        defined as how long the fuel has been out of the reactor core: $days$ 
        (or $years$).
\end{enumerate}

Next, the information reduction step is covered in Section
\ref{sec:inforeduc1}, where uniform error is randomly applied. After this, the
less-precise training data sets will be input to a statistical learner for the
next step: training models.

Section \ref{sec:statmodel1} details the implementation of the algorithms
introduced in \ref{sec:algs}. They use the features and labels in the training
data sets to formulate a model. 

After this, the algorithms must be evaluated for their prediction performance
when given test samples (i.e., a new \gls{SNF} measurement that has no labels
according the to algorithm).  The approach is shown in Section \ref{sec:eval1}.
First presented is the prediction performance of samples that are taken out of
the training data set to be used as test cases, which is shown in Section
\ref{sec:randerr}.  Second, an external test set with nuclide concentrations is
used to show how this approach performs with real world measurements. This is
discussed in Section \ref{sec:sfcompo}.  

\section{Training Data Simulation}
\label{sec:training1}
\input{chapters/exp1/training}

\section{Information Reduction}
\label{sec:inforeduc1}
\input{chapters/exp1/inforeduc}

\section{Statistical Learning Implementation}
\label{sec:statmodel1}
\input{chapters/exp1/statmodels}

\section{Prediction Performance Evaluation}
\label{sec:eval1}
\input{chapters/exp1/evaluation}

\section{Summary}

%This chapter covers the details of the methodology in four main sections:
%training set simulations, information reduction, statistical methods
%implementation, and performance evaluation. This approach focuses on the
%situation where there is a full set of well-measured nuclides, some of which
%require destructive techniques to measure, which will later be contrasted to a
%set of nuclides that are measured in a non-destructive manner. 
%
%First, in Section \ref{sec:training1}, the training data is simulated, which
%provides an array of nuclide measurements as the features. The prediction
%parameters are the simulation inputs: reactor type, burnup, \gls{U235}
%enrichment, and time since irradiation.  The reactor types are one of three
%common commercial reactors: \gls{PWR}, \gls{BWR}, or \gls{PHWR}.  The burnup is
%measured in $MWd/MTU$ and ranges from 0 to about $68,000$ in 21-28 timesteps
%depending on the reactor type.  The enrichment is $\%\:{}^{235}\text{U}$ and is
%centered around the following percentages: 0.5, 1.5, 2.0, 3.0, 4.0, 5.0, and
%0.711 (natural \gls{U235} enrichment for the \gls{PHWR}s).  The time since
%irradiation is measured in $days$ and ranges from 0 to 6000, or $16\:years$, in
%$100\:day$ time steps with 61 total steps. Given these simulation inputs, the
%makeup of $4.5 \times 10^5$ \gls{SNF} entries are simulated using \gls{ORIGEN}
%\cite{scale, origen, origenarp}.
%
%Second, in Section \ref{sec:inforeduc1}, information reduction on the training
%set is carried out using randomly injected uniform error. The random error is
%used to study the robustness of the methodology to artificial noise in the
%feature set, which is comprised of 29 nuclide masses.  The introduced training
%set error is increased up to 20\%.
%
%Third, in Section \ref{sec:statmodel1}, three algorithms, \textit{k}-nearest
%neighbors, decision trees, and \gls{MLL} calculations, are used to train models
%to predict the four reactor parameters of interest.  The first two are
%algorithms implemented using the scikit-learn python \gls{ML} toolkit, and
%\gls{MLL} is implemented in python leveraging SciPy and NumPy for fast
%likelihood calculations \cite{scikit, scipy, numpy}.  For the scikit
%algorithms, the hyperparameters governing model complexity were optimized to
%minimize the prediction errors.  The scripts written to run the scikit-learn
%predictions and the \gls{MLL} calculations were deployed using
%\gls{UW}--Madison's \gls{CHTC} resources, the \gls{UW} campus grid, and the
%\gls{OSG} \cite{osg07, osg09}.  
%
%Fourth, in Section \ref{sec:eval1}, the prediction errors are evaluated to draw
%conclusions about the capability of the chosen statistical methods to inform
%\gls{SNF} attribution with increasingly less precise material measurements.
%Sections \ref{sec:randerrA} and \ref{sec:randerrB} show the results of
%information reduction by injecting noise into the training set (randomly
%applied uniform error).  Next, the impacts of training set size are evaluated
%to understand the effects of model generalization in Section
%\ref{sec:randerrC}. After this, the impacts of having prior knowledge of the
%reactor type on the quality of prediction of the regression cases are studied
%in Section \ref{sec:randerrD}. Last, the external testing set, the
%\gls{SFCOMPO} database, is used to test this methodology in Section
%\ref{sec:sfcompo}. 
%
%For Sections \ref{sec:randerrA}--\ref{sec:randerrD}, the prediction performance
%is measured by using the training set to provide testing samples. The entire
%training set is tested at some point; when a test sample is used, it is first
%removed from the training set so that there is no exact replica of it in the
%training stage. For the scikit-learn algorithms, this is accomplished via
%5-fold \gls{CV}, and the \gls{MLL} calculations test one training set entry at
%a time.  Impacts of increasing training set error on the four prediction
%parameters are covered in Sections \ref{sec:randerrA} and \ref{sec:randerrB}
%The \gls{MLL} calculations method is the most resilient to introduced error,
%followed by decision trees then \textit{k}-nearest neighbors. The behaviors of
%each algorithm's performance degradation for reactor type, burnup, and
%enrichment all behave in a similar fashion. But the \textit{k}-nearest
%neighbors prediction of time since irradiation has a drastic drop that
%starts with even 1\% introduced training set error. The prediction performance
%at 20\% training set error is used as a baseline for future work.

This chapter covers the details of the methodology in four main sections:
training set simulations, information reduction, statistical methods
implementation, and performance evaluation. This approach mimics the situation
where there is a full set of well-measured nuclides, some of which require
destructive techniques to measure. 

First, in Section \ref{sec:training1}, the training data is simulated, which
provides an array of nuclide measurements as the features. The prediction
parameters are the simulation inputs: reactor type, burnup, \gls{U235}
enrichment, and time since irradiation.  Given the chosen inputs, the makeup of
$4.5 \times 10^5$ \gls{SNF} entries are simulated using \gls{ORIGEN}
\cite{scale, origen, origenarp}.

Second, in Section \ref{sec:inforeduc1}, information reduction on the training
set is carried out using randomly injected uniform error. The random error is
used to study the robustness of the methodology to artificial noise in the
feature set, which is comprised of 29 nuclide masses.  The introduced training
set error is increased up to 20\%.

Third, in Section \ref{sec:statmodel1}, three algorithms, \textit{k}-nearest
neighbors, decision trees, and \gls{MLL} calculations, are used to train models
to predict the four reactor parameters of interest.  The first two are
algorithms implemented using the scikit-learn python \gls{ML} toolkit, and
\gls{MLL} is implemented in python leveraging SciPy and NumPy for fast
likelihood calculations \cite{scikit, scipy, numpy}.  For the scikit
algorithms, the hyperparameters governing model complexity were optimized to
minimize the prediction errors.  

Fourth, in Section \ref{sec:eval1}, the prediction errors are evaluated to draw
conclusions about the capability of the chosen statistical methods to inform
\gls{SNF} attribution with increasingly less precise material measurements.
For Section \ref{sec:randerr}, the prediction performance is measured by using
the training set to provide testing samples. The entire training set is tested
at some point for all three algorithms.  Sections \ref{sec:randerrA} and
\ref{sec:randerrB} show the results of information reduction by injecting noise
into the training set (randomly applied uniform error).  Next, the impacts of
training set size are evaluated to understand the effects of model
generalization in Section \ref{sec:randerrC}. After this, the impacts of having
prior knowledge of the reactor type on the quality of prediction of the
regression cases are studied in Section \ref{sec:randerrD}.  Last, the external
testing set, the \gls{SFCOMPO} database, is used to test this methodology in
Section \ref{sec:sfcompo}. 

Sections \ref{sec:randerrA} and \ref{sec:randerrB} cover the impacts of
increasing training set error on the four prediction parameters.  The reactor
type classification shows a tendency for \glspl{PWR}s to be misclassified as
\gls{BWR}, and less so vice versa. The few misclassified \glspl{PHWR}s also
tend towards \gls{BWR}. Neither of these findings are surprising because
\gls{BWR} is the majority class. A more balanced training set and/or better
simulation fidelty would be required to improve these misclassifications.
Burnup, ${}^{235}\text{U}$ enrichment, and time since irradiation relative
prediction errors are all above -6\% (if the anomalous \textit{k}-nearest
neighbors results for time since irradiation are excluded), even at 20\%
training set error.

For all four prediction categories, the \gls{MLL} calculations method is the
most resilient to introduced error, followed by decision trees then
\textit{k}-nearest neighbors. Interestingly, at the 1\% training set error
level, \gls{MLL} calculations tend to do slightly worse than the scikit-learn
algorithms.  \todo[inline]{Paul brought up the upper bound performance of MLL
since the test case is removed. but 20\% of training set is removed for scikit
preds, so at least for $k=1$ the upper bound is potentially worse for kNN. I
think it's from the way the errors are applied, which I think I mentioned in
final conclusions} The behaviors of each algorithm's performance degradation
for reactor type, burnup, and enrichment all behave in a similar fashion. But
for time since irradiation, the \textit{k}-nearest neighbors prediction
performance has a drastic drop that starts with even 1\% introduced training
set error.  \textit{While it is difficult to draw a line and say which
algorithm's performance is acceptable or "good enough" (especially considering
the relative errors are all under 6\%), the prediction performances at 20\%
training set error are used as a minimum baseline for future work.}

In Section \ref{sec:randerrC}, the impacts of fewer training set entries are
implemented to investigate how the algorithms each generalize to unseen
samples. Using the training set that has 5\% error, it was reduced in four
steps from its full size, the lowest size being 20\% of the full training set.
Although the \gls{MLL} calculations perform above the scikit-learn algorithms
at most sizes, the data points at 20\% training set size show \gls{MLL} below
one or both of the scikit-learn algorithms. \textit{The takeaway is that while
this training set is more than large enough to achieve good performance from a
high variance approach, the performance may not hold with a different training
set design.}

Section \ref{sec:randerrD} shows whether the burnup, enrichment, and time since
irradiation predictions benefit from having the prior knowledge of reactor
type.  The largest improvement is for the \gls{PHWR} (only 1.5\% of training
set) regression cases for the scikit-learn algorithms, whereas there is no
improvement for \gls{MLL} calculations. Because the \gls{BWR} class dominates
the training set, there is modest improvement in \gls{PWR} regression cases as
well for the scikit-learn algorithms. \textit{These results indicate regression
cases linked to \gls{PHWR}s may not perform well.} 

Last in this chapter is Section \ref{sec:sfcompo}, where the performance of
this methodology using real world test cases of nuclide concentration
measurements via the \gls{SFCOMPO} database is demonstrated.  While the
training set design spans the label space that exists in \gls{SFCOMPO}, there
are many missing measurements from the features in the training set, which is
comprised of 29 nuclide masses.  Because of an imbalanced training set and the
method used to handle the null values, the reactor type classification results
are mostly poor (except for the \gls{MLL} calculations with the zero-imputed
nulls testing set with a balanced accuracy score of 0.63), which extends to the
regression cases. It is again true that investigating relative error statistics
alongside absolute error statistics provides a fuller picture of the regression
of the \gls{SFCOMPO} database entries.  The \gls{MLL} calculations perform the
best for burnup and the decision trees perform the best for ${}^{235}\text{U}$
enrichment (both with the zero-imputed nulls testing set), but many of the
prediction errors are still large. An investigation into the true versus
predicted values (especially for burnup) shows that \textit{the mean-imputed
nulls database is clearly not worth further investigation, but much improvement
could be made to the zero-imputed nulls method}.

It is possible that because most of the 505 entries contain many of the
of-interest uranium and plutonium isotopes, that this testing set should be
used only in a study limited to those isotopes. They are known to provide good
discrimination on their own \cite{pu_discrimination, nicolaou_2006,
nicolaou_pu, nicolaou_2009, nicolaou_2014, nicolaou_2015}.  There are two areas
for improvement: new approaches to testing set features (using only plutonium
and uranium isotopes, different approaches to null value imputation) and
improving the simulation fidelity of the training set. \textit{The
\gls{SFCOMPO} database has challenges to being used as an external testing set
with this methodology, but could be improved with more consideration.}

